{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Competition Objective is to detect fraud in transactions;\n\n### Data\nIn this competition, you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target 'isFraud'.\n\nThe data is broken into two files 'identity' and 'transaction', which are joined by 'TransactionID'.\n\n> Note : Not all transactions have corresponding identity information.\n\n##### Categorical Features-Transaction\n* ProductCD\n* emaildomain\n* card1-card6\n* addr1,addr2\n* P_emaildomain\n* R_emaildomain\n* M1-M9\n\n##### Categorical Features-Identity\n* DeviceType\n* DeviceInfo\n* id_12-id_38","metadata":{}},{"cell_type":"markdown","source":"### Questions\n1. What type of data we have on our data?\n2. How many cols, rows, missing values we have?\n3. What's the target distribution?\n4. What's the transactions values distribution of fraud and no fraud transactions?\n5. We have predominant fraudulent products?\n6. What features or target shows some interesting patterns?\n7. A lot of more questions that will raise throughout the exploration.","metadata":{}},{"cell_type":"markdown","source":"### Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standard plotly imports\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n# import cufflinks\n# import cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\n# cufflinks.go_offline(connected=True)\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score,KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n# Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nimport os\nimport gc\nprint(os.listdir(\"../input\"))","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:48:59.298153Z","iopub.execute_input":"2021-06-27T04:48:59.298578Z","iopub.status.idle":"2021-06-27T04:49:05.202963Z","shell.execute_reply.started":"2021-06-27T04:48:59.298473Z","shell.execute_reply":"2021-06-27T04:49:05.201471Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"name":"stdout","text":"['ieee-fraud-detection']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Importing train datasets","metadata":{}},{"cell_type":"code","source":"df_id=pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:49:16.641076Z","iopub.execute_input":"2021-06-27T04:49:16.641411Z","iopub.status.idle":"2021-06-27T04:49:17.437047Z","shell.execute_reply.started":"2021-06-27T04:49:16.641377Z","shell.execute_reply":"2021-06-27T04:49:17.436194Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df_trans=pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:49:20.160485Z","iopub.execute_input":"2021-06-27T04:49:20.160873Z","iopub.status.idle":"2021-06-27T04:49:47.461971Z","shell.execute_reply.started":"2021-06-27T04:49:20.160825Z","shell.execute_reply":"2021-06-27T04:49:47.461094Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary=pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary=summary.reset_index()\n    summary['Name']=summary['index']\n    summary=summary[['Name','dtypes']]\n    summary['Missing']=df.isnull().sum().values\n    summary['Uniques']=df.nunique().values\n    summary['First Value']=df.loc[0].values\n    summary['Second Value']=df.loc[1].values\n    summary['Third Value']=df.loc[2].values\n    \n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name']==name,'Entropy']=round(stats.entropy(df[name].value_counts(normalize=True),base=2),2)\n        \n    return summary","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:50:17.363795Z","iopub.execute_input":"2021-06-27T04:50:17.364139Z","iopub.status.idle":"2021-06-27T04:50:17.371585Z","shell.execute_reply.started":"2021-06-27T04:50:17.364108Z","shell.execute_reply":"2021-06-27T04:50:17.370615Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"## Function to reduce DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:50:19.563149Z","iopub.execute_input":"2021-06-27T04:50:19.563460Z","iopub.status.idle":"2021-06-27T04:50:19.575779Z","shell.execute_reply.started":"2021-06-27T04:50:19.563431Z","shell.execute_reply":"2021-06-27T04:50:19.574606Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def CalcOutliers(df_num):\n    \n    # calculating mean and std of the array\n    data_mean,data_std=np.mean(df_num),np.std(df_num)\n    \n    # setting the cut line to both higher and lower values\n    cut=data_std*3\n    \n    # Caculating the higher and lower cut values\n    lower,upper=data_mean-cut,data_mean+cut\n    \n    # creating an array of lower, higher and total outlier values\n    outliers_lower=[x for x in df_num if x<lower]\n    outliers_higher=[x for x in df_num if x>upper]\n    outliers_total=[x for x in df_num if x<lower or x>upper]\n    \n    # array without outlier values\n    outliers_removed=[x for x in df_num if x>lower and x<upper]\n    \n    # printing total number of values in lower cut of outliers\n    print('Identified lowest outliers: %d' % len(outliers_lower))\n    # printing total number of values in higher cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher))\n    # printing total number of values outliers of both sides\n    print('Total outliers observations: %d' % len(outliers_total))\n    # printing total number of non-outlier values\n    print('Non-outlier observations: %d' % len(outliers_removed))\n    # percentual of outliers in points\n    print('Total precentual of Outliers: ', round((len(outliers_total)/len(outliers_removed))*100,4))\n    \n    return","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:50:22.143183Z","iopub.execute_input":"2021-06-27T04:50:22.143500Z","iopub.status.idle":"2021-06-27T04:50:22.151322Z","shell.execute_reply.started":"2021-06-27T04:50:22.143469Z","shell.execute_reply":"2021-06-27T04:50:22.150048Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Reducing memory\ndf_trans=reduce_mem_usage(df_trans)\ndf_id=reduce_mem_usage(df_id)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:50:24.913102Z","iopub.execute_input":"2021-06-27T04:50:24.913417Z","iopub.status.idle":"2021-06-27T04:52:05.723565Z","shell.execute_reply.started":"2021-06-27T04:50:24.913385Z","shell.execute_reply":"2021-06-27T04:52:05.722640Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Mem. usage decreased to 542.35 Mb (69.4% reduction)\nMem. usage decreased to 25.86 Mb (42.7% reduction)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Knowing the data","metadata":{}},{"cell_type":"code","source":"resumetable(df_trans)[:25]","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:32:36.064152Z","iopub.execute_input":"2021-06-26T06:32:36.064521Z","iopub.status.idle":"2021-06-26T06:32:47.062575Z","shell.execute_reply.started":"2021-06-26T06:32:36.064486Z","shell.execute_reply":"2021-06-26T06:32:47.061734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target Distribution","metadata":{}},{"cell_type":"code","source":"df_trans['TransactionAmt']=df_trans['TransactionAmt'].astype(float)\ntotal=len(df_trans)\ntotal_amt=df_trans.groupby(['isFraud'])['TransactionAmt'].sum().sum()\nplt.figure(figsize=(16,6))\n\nplt.subplot(121)\ng=sns.countplot(x='isFraud',data=df_trans,)\ng.set_title(\"Fraud Transactions Distribution \\n# 0: No Fraud | 1: Fraud #\",fontsize=22)\ng.set_xlabel(\"Is Fraud?\",fontsize=18)\ng.set_ylabel(\"Count\",fontsize=18)\n\nfor p in g.patches:\n    height=p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,height+3,\"{:1.2f}%\".format(height/total*100),ha=\"center\",fontsize=15)\n    \n    perc_amt=(df_trans.groupby(['isFraud'])['TransactionAmt'].sum())\n    perc_amt=perc_amt.reset_index()\n    plt.subplot(122)\n    g1=sns.barplot(x='isFraud',y='TransactionAmt',dodge=True,data=perc_amt)\n    g1.set_title(\"% Total Amount in Transaction Amt \\n# 0: No Fraud | 1: Fraud #\",fontsize=22)\n    g1.set_xlabel(\"Is fraud?\",fontsize=18)\n    g1.set_ylabel('Total Transaction Amount Scalar',fontsize=18)\n    for p in g1.patches:\n        height=p.get_height()\n        g1.text(p.get_x()+p.get_width()/2.,height+3,'{:1.2f}%'.format(height/total_amt*100),ha=\"center\",fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:32:52.316704Z","iopub.execute_input":"2021-06-26T06:32:52.317068Z","iopub.status.idle":"2021-06-26T06:32:52.756048Z","shell.execute_reply.started":"2021-06-26T06:32:52.317035Z","shell.execute_reply":"2021-06-26T06:32:52.755055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 3.5% of Fraud transaction in out datasets.\nI think that it would be interesting to see if the amount of percentuals is higher or lower than 3.5% of total.","metadata":{}},{"cell_type":"markdown","source":"### Transaction Amount Quantiles\n\nBefore ploting the Transaction Amount, let's see the quantiles of Transaction Amount.","metadata":{}},{"cell_type":"code","source":"df_trans['TransactionAmt']=df_trans['TransactionAmt'].astype(float)\nprint(\"Transaction Amounts Quatiles:\")\nprint(df_trans['TransactionAmt'].quantile([.01,.025,.1,.25,.5,.75,.9,.975,.99]))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:33:00.024166Z","iopub.execute_input":"2021-06-26T06:33:00.02451Z","iopub.status.idle":"2021-06-26T06:33:00.055452Z","shell.execute_reply.started":"2021-06-26T06:33:00.024477Z","shell.execute_reply":"2021-06-26T06:33:00.054391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ploting Transaction Amount Values Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,12))\nplt.suptitle('Transaction Values Distribution', fontsize=22)\nplt.subplot(221)\ng=sns.distplot(df_trans[df_trans['TransactionAmt']<=1000]['TransactionAmt'])\ng.set_title(\"Transaction Amount Distribution<=1000\",fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\",fontsize=15)\n\nplt.subplot(222)\ng1=sns.distplot(np.log(df_trans[\"TransactionAmt\"]))\ng1.set_title(\"Transaction Amount (Log) Distribution\",fontsize=18)\ng1.set_xlabel(\"\")\ng1.set_ylabel(\"Probability\",fontsize=15)\n\nplt.figure(figsize=(16,12))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:33:05.886396Z","iopub.execute_input":"2021-06-26T06:33:05.886755Z","iopub.status.idle":"2021-06-26T06:33:12.589795Z","shell.execute_reply.started":"2021-06-26T06:33:05.886723Z","shell.execute_reply":"2021-06-26T06:33:12.589005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(212)\ng4=plt.scatter(range(df_trans[df_trans['isFraud']==0].shape[0]),\n               np.sort(df_trans[df_trans['isFraud']==0]['TransactionAmt'].values),\n               label=\"Fraud\",alpha=.2)\ng4=plt.scatter(range(df_trans[df_trans['isFraud']==1].shape[0]),\n              np.sort(df_trans[df_trans['isFraud']==1]['TransactionAmt'].values),\n              label='Fraud',alpha=.2)\ng4=plt.title(\"ECDF \\nFRAUD and NO FRAUD Transaction Amount Distribution\",fontsize=15)\ng4=plt.xlabel(\"Index\")\ng4=plt.ylabel(\"Amount Distribution\",fontsize=10)\ng4=plt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:34:00.118767Z","iopub.execute_input":"2021-06-26T06:34:00.11911Z","iopub.status.idle":"2021-06-26T06:34:07.919388Z","shell.execute_reply.started":"2021-06-26T06:34:00.119078Z","shell.execute_reply":"2021-06-26T06:34:07.918532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,12))\n\nplt.subplot(322)\ng1=plt.scatter(range(df_trans[df_trans['isFraud']==0].shape[0]),\n              np.sort(df_trans[df_trans['isFraud']==0]['TransactionAmt'].values),\n              label='NoFraud',alpha=.2)\ng1=plt.title(\"NO FRAUD - Transaction Amount ECDF\",fontsize=18)\ng1=plt.xlabel(\"Index\")\ng1=plt.ylabel(\"Amount Distribution\",fontsize=15)\n\nplt.suptitle('Individual ECDF Distribution',fontsize=22)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:34:23.778083Z","iopub.execute_input":"2021-06-26T06:34:23.778457Z","iopub.status.idle":"2021-06-26T06:34:27.960607Z","shell.execute_reply.started":"2021-06-26T06:34:23.778423Z","shell.execute_reply":"2021-06-26T06:34:27.959787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seeing the Quantiles of Fraud and No Fraud Transactions","metadata":{}},{"cell_type":"code","source":"print(pd.concat([df_trans[df_trans['isFraud']==1]['TransactionAmt'].quantile([.01,.1,.25,.5,.75,.9,.99]).reset_index(),\n     df_trans[df_trans['isFraud']==0]['TransactionAmt'].quantile([.01,.1,.25,.5,.75,.9,.99]).reset_index()],\n     axis=1,keys=['Fraud','No Fraud']))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:34:42.178001Z","iopub.execute_input":"2021-06-26T06:34:42.178349Z","iopub.status.idle":"2021-06-26T06:34:43.540298Z","shell.execute_reply.started":"2021-06-26T06:34:42.178314Z","shell.execute_reply":"2021-06-26T06:34:43.539211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transaction Amount Outliers","metadata":{}},{"cell_type":"code","source":"CalcOutliers(df_trans['TransactionAmt'])","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:34:49.087543Z","iopub.execute_input":"2021-06-26T06:34:49.087934Z","iopub.status.idle":"2021-06-26T06:34:49.885349Z","shell.execute_reply.started":"2021-06-26T06:34:49.087901Z","shell.execute_reply":"2021-06-26T06:34:49.884398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If wer consider only values between 0 to 800, we will avoid the outliers and has more confidence in our distribution. We have 10k rows with outliers that represents 1.74% of total rows.","metadata":{}},{"cell_type":"markdown","source":"## Now, let's know the Product Feature\n\n* Distribution Products\n* Distribution of Frauds by Product\n* Has Difference between Transaction Amounts in Products?","metadata":{}},{"cell_type":"code","source":"tmp=pd.crosstab(df_trans['ProductCD'],df_trans['isFraud'],normalize='index')*100\ntmp=tmp.reset_index()\ntmp.rename(columns={0:'NoFraud',1:'Fraud'},inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('ProductCD Distribution',fontsize=22)\n\nplt.subplot(221)\ng=sns.countplot(x='ProductCD',data=df_trans)\n# plt.legend(title='Fraud',loc='upper center',labels=['No','Yes'])\n\ng.set_title(\"ProductCD Distribution\",fontsize=19)\ng.set_xlabel(\"ProductCD Name\",fontsize=17)\ng.set_ylabel(\"Count\",fontsize=17)\ng.set_ylim(0,500000)\nfor p in g.patches:\n    height=p.get_height()\n    g.text(p.get_x()+p.get_width()/2,height+3,'{:1.2f}%'.format(height/total*100),ha='center',fontsize=14)\n    \nplt.subplot(222)\ng1=sns.countplot(x='ProductCD',hue='isFraud',data=df_trans)\nplt.legend(title='Fraud',loc=\"best\",labels=[\"No\",\"Yes\"])\ngt=g1.twinx()\ngt=sns.pointplot(x='ProductCD',y='Fraud',data=tmp,color='black',order=['W','H','C','S','R'],lengend=False)\ngt.set_ylabel(\"% of Fraud Transactions\",fontsize=16)\n\ng1.set_title(\"Product CD by Target(isFraud)\",fontsize=19)\ng1.set_xlabel(\"ProductCD Name\",fontsize=17)\ng1.set_ylabel(\"Count\",fontsize=17)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:35:11.967479Z","iopub.execute_input":"2021-06-26T06:35:11.967873Z","iopub.status.idle":"2021-06-26T06:35:13.91837Z","shell.execute_reply.started":"2021-06-26T06:35:11.967839Z","shell.execute_reply":"2021-06-26T06:35:13.917466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(212)\ng3=sns.boxenplot(x='ProductCD',y='TransactionAmt',hue='isFraud',data=df_trans[df_trans['TransactionAmt']<=2000])\ng3.set_xlabel('ProductCD Name',fontsize=17)\ng3.set_ylabel(\"Transaction Values\",fontsize=17)\n\nplt.subplots_adjust(hspace=0.6,top=0.85)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:35:17.570856Z","iopub.execute_input":"2021-06-26T06:35:17.571204Z","iopub.status.idle":"2021-06-26T06:35:19.644369Z","shell.execute_reply.started":"2021-06-26T06:35:17.571172Z","shell.execute_reply":"2021-06-26T06:35:19.64338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"W,C, and R are the most frequent values.\nWe can note that in W,H and R the distribution of Fraud values are slightly higher than the Non-Fraud Transactions.","metadata":{}},{"cell_type":"markdown","source":"## Card Features\n* Card features are categoricals.\n* Let's understand the distribution of values.\n* What's the difference in transactions and % of Fraud for each values in these features.\n* Card features has 6 columns, and 4 of them seems to be numericals, so let's see the quantiles and distributions.","metadata":{}},{"cell_type":"code","source":"## Knowing the Card Features\nresumetable(df_trans[['card1','card2','card3','card4','card5','card6']])","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:35:32.715099Z","iopub.execute_input":"2021-06-26T06:35:32.715452Z","iopub.status.idle":"2021-06-26T06:35:33.265014Z","shell.execute_reply.started":"2021-06-26T06:35:32.715418Z","shell.execute_reply":"2021-06-26T06:35:33.26395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Card2-Card6 has some missing values. We will need to due with it later.","metadata":{}},{"cell_type":"markdown","source":"## Numerical Feature Card Quantiles","metadata":{}},{"cell_type":"code","source":"print(\"Card Features Quantiles:\")\nprint(df_trans[['card1','card2','card3','card5']].quantile([0.01,0.025,.1,.25,.5,.75,.975,.99]))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:35:39.336908Z","iopub.execute_input":"2021-06-26T06:35:39.337324Z","iopub.status.idle":"2021-06-26T06:35:39.423275Z","shell.execute_reply.started":"2021-06-26T06:35:39.33729Z","shell.execute_reply":"2021-06-26T06:35:39.4223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that Card 1 and Card 2 has a large distribution of values, so maybe it will be better to get the log of these columns.","metadata":{}},{"cell_type":"code","source":"df_trans.loc[df_trans.card3.isin(df_trans.card3.value_counts()[df_trans.card3.value_counts()<200].index),'card3']=\"Others\"\ndf_trans.loc[df_trans.card5.isin(df_trans.card5.value_counts()[df_trans.card5.value_counts()<300].index),'card5']=\"Others\"","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:35:44.176031Z","iopub.execute_input":"2021-06-26T06:35:44.176385Z","iopub.status.idle":"2021-06-26T06:35:44.801902Z","shell.execute_reply.started":"2021-06-26T06:35:44.17635Z","shell.execute_reply":"2021-06-26T06:35:44.800948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Card 1, Card 2 and Card 3 Distributions\n* As Card 1 and 2 are numericals, I will plot the distribution of them.\n* In Card 3, as we have many values with low frequencies, I decided to set value to \"Others\".\n* Also, in Card 3, I set the % of Fraud ratio in yaxis2.","metadata":{}},{"cell_type":"code","source":"tmp=pd.crosstab(df_trans['card3'],df_trans['isFraud'],normalize='index')*100\ntmp=tmp.reset_index()\ntmp.rename(columns={0:'NoFraud',1:'Fraud'},inplace=True)\n\ntmp2=pd.crosstab(df_trans['card5'],df_trans['isFraud'],normalize='index')*100\ntmp2=tmp2.reset_index()\ntmp2.rename(columns={0:'NoFraud',1:'Fraud'},inplace=True)\n\nplt.figure(figsize=(14,22))\n\nplt.subplot(411)\ng=sns.distplot(df_trans[df_trans['isFraud']==1]['card1'],label='Fraud')\ng=sns.distplot(df_trans[df_trans['isFraud']==0]['card1'],label='NoFraud')\ng.legend()\ng.set_title(\"Card 1 Values Distribution by Target\",fontsize=20)\ng.set_xlabel(\"Card 1 Values\",fontsize=18)\ng.set_ylabel(\"Probability\",fontsize=18)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:36:08.989476Z","iopub.execute_input":"2021-06-26T06:36:08.989888Z","iopub.status.idle":"2021-06-26T06:36:13.707557Z","shell.execute_reply.started":"2021-06-26T06:36:08.989854Z","shell.execute_reply":"2021-06-26T06:36:13.706758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(412)\ng1=sns.distplot(df_trans[df_trans['isFraud']==1]['card2'],label='Fraud')\ng1=sns.distplot(df_trans[df_trans['isFraud']==0]['card2'],label='NoFraud')\ng1.legend()\ng1.set_title(\"Card 2 Values Distribution by Target\",fontsize=20)\ng1.set_xlabel(\"Card 2 Values\",fontsize=18)\ng1.set_ylabel(\"Probability\",fontsize=18)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:36:16.946275Z","iopub.execute_input":"2021-06-26T06:36:16.94665Z","iopub.status.idle":"2021-06-26T06:36:21.440773Z","shell.execute_reply.started":"2021-06-26T06:36:16.946612Z","shell.execute_reply":"2021-06-26T06:36:21.439977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(413)\ng2 = sns.countplot(x='card3',data=df_trans,order=list(tmp.card3.values))\ng22 = g2.twinx()\ngg2 = sns.pointplot(x='card3',y='Fraud',data=tmp,color='black',order=list(tmp.card3.values))\ngg2.set_ylabel(\"% of Fraud Transactions\",fontsize=16)\ng2.set_title(\"Card 3 Values Distribution and % of Transaction Frauds\",fontsize=20)\ng2.set_xlabel(\"Card 3 Values\",fontsize=18)\ng2.set_ylabel(\"Count\",fontsize=18)\nfor p in g2.patches:\n    height = p.get_height()\n    g2.text(p.get_x()+p.get_width()/2,height + 25,'{:1.2f}%'.format(height/total*100),ha=\"center\") ","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:36:37.005595Z","iopub.execute_input":"2021-06-26T06:36:37.00594Z","iopub.status.idle":"2021-06-26T06:36:37.613307Z","shell.execute_reply.started":"2021-06-26T06:36:37.005908Z","shell.execute_reply":"2021-06-26T06:36:37.612466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(414)\ng3=sns.countplot(x='card5',data=df_trans,order=list(tmp2.card5.values))\ng3t=g3.twinx()\ng3t=sns.pointplot(x='card5',y='Fraud',data=tmp2,color='black',order=list(tmp2.card5.values))\ng3t.set_ylabel(\"% of Fraud Transactions\",fontsize=16)\ng3.set_title(\"Card 5 Values Distribution and % of Transaction Frauds\",fontsize=20)\ng3.set_xticklabels(g3.get_xticklabels(),rotation=90)\ng3.set_xlabel(\"Card 5 Values\",fontsize=18)\ng3.set_ylabel(\"Count\",fontsize=18)\nfor p in g3.patches:\n    height=p.get_height()\n    g3.text(p.get_x()+p.get_width()/2,height+3,'{:1.2f}%'.format(height/total*100),ha='center',fontsize=11)\nplt.subplots_adjust(hspace=0.6,top=0.85)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In Card 3, we can see that 150 and 185 are the most common values in the column.\nWe have over 12% of Fraud in 185 and about 3% in 150; The values with highest Fraud Transactions are 185 and 119.\n\nIn Card 5, the most frequent values are 226, 224 and 166 that represent 73% of data. Also it's possible to see high % of frauds in 137, 147 and 141 that has few entries for values.","metadata":{}},{"cell_type":"markdown","source":"## Card 4 - Categorical","metadata":{}},{"cell_type":"code","source":"tmp=pd.crosstab(df_trans['card4'],df_trans['isFraud'],normalize='index')*100\ntmp=tmp.reset_index()\ntmp.rename(columns={0:\"NoFraud\",1:\"Fraud\"},inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle(\"Card 4 Distribution\",fontsize=22)\n\nplt.subplot(221)\ng=sns.countplot(x='card4',data=df_trans)\n# plt.legend(title=\"Fraud\",loc=\"upper center\",labels=[\"No\",\"Yes\"])\ng.set_title(\"Card 4 Distribution\",fontsize=19)\ng.set_ylim(0,420000)\ng.set_xlabel(\"Card 4 Category Names\",fontsize=17)\ng.set_ylabel(\"Count\",fontsize=17)\nfor p in g.patches:\n    height=p.get_height()\n    g.text(p.get_x()+p.get_width()/2,height+3,\"{:1.2f}%\".format(height/total*100),ha=\"center\",fontsize=14)\n    \nplt.subplot(222)\ng1=sns.countplot(x=\"card4\",hue=\"isFraud\",data=df_trans)\nplt.legend(title=\"Fraud\",loc=\"best\",labels=[\"No\",\"Yes\"])\ngt=g1.twinx()\ngt=sns.pointplot(x='card4',y='Fraud',data=tmp,color='black',legend=False,order=['discover','mastercard','visa','american express'])\ng1.set_ylabel(\"% of Fraud Transaction\",fontsize=16)\ng1.set_title(\"Card4 by Target(isFraud)\",fontsize=19)\ng1.set_xlabel(\"Card4 Category Names\",fontsize=17)\ng1.set_ylabel(\"Count\",fontsize=17)\n\nplt.subplot(212)\ng3=sns.boxenplot(x='card4',y='TransactionAmt',hue='isFraud',data=df_trans[df_trans['TransactionAmt']<=2000])\ng3.set_title(\"Card 4 Distribution by ProductCD and Target\",fontsize=20)\ng3.set_xlabel(\"Card 4 Category Names\",fontsize=17)\ng3.set_ylabel(\"Transaction Values\",fontsize=17)\n\nplt.subplots_adjust(hspace=0.6,top=0.85)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:17:41.711992Z","iopub.execute_input":"2021-06-26T06:17:41.712341Z","iopub.status.idle":"2021-06-26T06:17:45.611838Z","shell.execute_reply.started":"2021-06-26T06:17:41.712304Z","shell.execute_reply":"2021-06-26T06:17:45.610912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that 97% of our data are in Mastercard(32%) and Visa(65%).\n\nWe have a highest value in discover(~8%) against ~3.5% of Mastercard and Visa and 2.87% in American Express.","metadata":{}},{"cell_type":"markdown","source":"## Card 6 - Categorical","metadata":{}},{"cell_type":"code","source":"tmp=pd.crosstab(df_trans['card6'],df_trans['isFraud'],normalize='index')*100\ntmp=tmp.reset_index()\ntmp.rename(columns={0:\"NoFraud\",1:\"Fraud\"},inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('Card 6 Distributions',fontsize=22)\n\nplt.subplot(221)\ng=sns.countplot(x='card6',data=df_trans,order=list(tmp.card6.values))\n# plt.legend(title=\"Fraud\",loc='upper center',labels=['No','Yes'])\ng.set_title(\"Card 6 Distribution\",fontsize=19)\ng.set_ylim(0,480000)\ng.set_xlabel(\"Card 6 Category Names\",fontsize=17)\ng.set_ylabel(\"Count\",fontsize=17)\nfor p in g.patches:\n    height=p.get_height()\n    g.text(p.get_x()+p.get_width()/2,height+3,\"{:1.2f}%\".format(height/total*100),ha='center',fontsize=14)\n    \nplt.subplot(222)\ng1=sns.countplot(x='card6',hue='isFraud',data=df_trans,order=list(tmp.card6.values))\nplt.legend(title='Fraud',loc='best',labels=['No','Yes'])\ngt=g1.twinx()\ng1=sns.pointplot(x='card6',y='Fraud',data=tmp,order=list(tmp.card6.values),color='black',legend=False)\ngt.set_ylim(0,20)\ng1.set_ylabel(\"% of Fraud Transactions\",fontsize=16)\ng1.set_title(\"Card 6 by Target(isFraud)\",fontsize=19)\ng1.set_xlabel(\"Card 6 Category Names\",fontsize=17)\ng1.set_ylabel(\"Count\",fontsize=17)\n\nplt.subplot(212)\ng3=sns.boxenplot(x='card6',y='TransactionAmt',hue='isFraud',data=df_trans[df_trans['TransactionAmt']<=2000],order=list(tmp.card6.values))\ng3.set_title(\"Card 6 Distribution by ProductCD and Target\")\ng3.set_xlabel(\"Card 6 Category Names\",fontsize=17)\ng3.set_ylabel(\"Transaction Values\",fontsize=17)\n\nplt.subplots_adjust(hspace=0.6,top=0.85)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:17:45.613301Z","iopub.execute_input":"2021-06-26T06:17:45.613645Z","iopub.status.idle":"2021-06-26T06:17:49.216494Z","shell.execute_reply.started":"2021-06-26T06:17:45.61361Z","shell.execute_reply":"2021-06-26T06:17:49.215586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All data is on Credit and Debit. We can see a high percentual of Frauds in Credit than Debit transactions.\n\nThe Distribution of Transaction Amoutn doesn't show clear differences.","metadata":{}},{"cell_type":"markdown","source":"## Exploring M1-M9 Features","metadata":{}},{"cell_type":"code","source":"for col in ['M'+str(i) for i in range(1,10)]:\n    df_trans[col]=df_trans[col].fillna('Miss')\n    \ndef plotting_dist_ratio(df,col,lim=2000):\n    tmp=pd.crosstab(df[col],df['isFraud'],normalize=\"index\")*100\n    tmp=tmp.reset_index()\n    tmp.rename(columns={0:\"NoFraud\",1:\"Fraud\"},inplace=True)\n    \n    plt.figure(figsize=(20,5))\n    plt.suptitle(f'{col} Distributions ',fontsize=22)\n    \n    plt.subplot(121)\n    g=sns.countplot(x=col,data=df,order=list(df[col].values))\n    # plt.legend(title=\"Fraud\",loc=\"upper center\",labels=['No','Yes'])\n    g.set_title(f'{col} Distribution\\nCount and % Fraud by each Category',fontsize=18)\n    g.set_ylim(0,400000)\n    gt=g.twinx()\n    gt=sns.pointplot(x=col,y='Fraud',data=tmp,order=list(df[col].values),color=\"black\",legend=False)\n    gt.set_ylim(0,20)\n    gt.set_ylabel(\"% of Fraud Transactions\",fontsize=16)\n    g.set_xlabel(f\"{col} Category Names\",fontsize=16)\n    g.set_ylabel(\"Count\",fontsize=17)\n    for p in gt.patches:\n        height=p.get_height()\n        gt.text(p.get_x()+p.get_width()/2,height+3,\"{:1.2f}%\".format(height/total*100),ha='center',fontsize=14)\n    \n    perc_amt=(df_trans.groupby(['isFraud',col])['TransactionAmt'].sum()/total_amt*100).unstack('isFraud')\n    perc_amt=per_amt.reset_index()\n    perc_amt.rename(columns={0:\"NoFraud\",1:\"Fraud\"},inplace=True)\n    \n    plt.subplot(122)\n    g1=sns.boxplot(x=col,y='TransactionAmt',hue='isFraud',data=df[df['TransactionAmt']<=lim],order=list(tmp[col].values))\n    g1t=g1.twinx()\n    g1t=sns.pointplot(x=col,y='Fraud',data=perc_amt,order=list(tmp[col].values))\n    g1t.set_ylim(0,5)\n    g1t.set_ylabel(\"% Fraud Total Amount\",fontsize=16)\n    g1.set_title(f\"{col} by Transactions dist\",fontsize=18)\n    g1.set_xlabel(f\"{col} Categorical Names\",fontsize=16)\n    g1.set_ylabel(\"Transaction Amount(U$)\",fontsize=16)\n    \n    plt.subplots_adjust(hspace=.4,wspace=0.35,top=0.8)\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:42:54.622202Z","iopub.execute_input":"2021-06-26T06:42:54.6227Z","iopub.status.idle":"2021-06-26T06:42:55.174451Z","shell.execute_reply.started":"2021-06-26T06:42:54.62265Z","shell.execute_reply":"2021-06-26T06:42:55.173449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### M distributions : Count, % Fraud and Transaction Amount Distribution","metadata":{}},{"cell_type":"code","source":"for col in ['M'+str(i) for i in range(1,10)]:\n    plotting_dist_ratio(df_trans,col,lim=2500)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:43:14.797104Z","iopub.execute_input":"2021-06-26T06:43:14.79745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only in M4 the Missing values isn't the highest % of Fraud.","metadata":{}},{"cell_type":"markdown","source":"### Addr1 and Addr2","metadata":{}},{"cell_type":"code","source":"print(\"Card Features Quantiles: \")\nprint(df_trans[['addr1','addr2']].quantile([0.01,.025,.1,.25,.5,.75,.90,.975,.99]))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:54:21.945538Z","iopub.execute_input":"2021-06-26T06:54:21.945953Z","iopub.status.idle":"2021-06-26T06:54:22.299926Z","shell.execute_reply.started":"2021-06-26T06:54:21.945919Z","shell.execute_reply":"2021-06-26T06:54:22.298779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will set all values in Addr1 that has less than 5000 entries to \"Others\".\n\nIn Addr2, I will set as \"Others\" all values with less than 50 entries.","metadata":{}},{"cell_type":"code","source":"df_trans.loc[df_trans.addr1.isin(df_trans.addr1.value_counts()[df_trans.addr1.value_counts()<=5000].index),'addr1']=\"Others\"\ndf_trans.loc[df_trans.addr2.isin(df_trans.addr2.value_counts()[df_trans.addr2.value_counts()<=50].index),'addr2']=\"Others\"","metadata":{"execution":{"iopub.status.busy":"2021-06-26T06:58:51.518965Z","iopub.execute_input":"2021-06-26T06:58:51.519428Z","iopub.status.idle":"2021-06-26T06:58:52.319417Z","shell.execute_reply.started":"2021-06-26T06:58:51.51939Z","shell.execute_reply":"2021-06-26T06:58:52.318505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Addr1 Distributions","metadata":{}},{"cell_type":"code","source":"def ploting_cnt_amt(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{col} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\",fontsize=12) \n        \n    g.set_ylim(0,max(sizes)*1.15)\n    \n    perc_amt = (df.groupby(['isFraud',col])['TransactionAmt'].sum() \\\n                / df.groupby([col])['TransactionAmt'].sum() * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    amt = df.groupby([col])['TransactionAmt'].sum().reset_index()\n    perc_amt = perc_amt.fillna(0)\n    plt.subplot(212)\n    g1 = sns.barplot(x=col, y='TransactionAmt', \n                       data=amt, \n                       order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, \n                        order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,perc_amt['Fraud'].max()*1.1)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    g1.set_title(f\"{col} by Transactions Total + %of total and %Fraud Transactions\", fontsize=20)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Total Amount(U$)\", fontsize=16)\n    g1.set_xticklabels(g.get_xticklabels(),rotation=45)    \n    \n    for p in g1.patches:\n        height = p.get_height()\n        g1.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total_amt*100),\n                ha=\"center\",fontsize=12) \n        \n    plt.subplots_adjust(hspace=.4, top = 0.9)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ploting_cnt_amt(df_trans, 'addr1')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Addr2 Distributions","metadata":{}},{"cell_type":"code","source":"ploting_cnt_amt(df_trans, 'addr2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost all entries in Addr2 are in the same value.\n\nInterestingly in the value 65 , the percent of frauds is almost 60%.\n\nAlthough the value 87 has 88% of total entries, it has 96% of Total Transaction Amounts.","metadata":{}},{"cell_type":"markdown","source":"### P emaildomain Distributions\n* I will group all e-mail domains by the respective enterprises.\n* Also, I wil lset as \"Others\" all values with less than 500 entries.","metadata":{}},{"cell_type":"code","source":"df_trans.loc[df_trans['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n\ndf_trans.loc[df_trans['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                         'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                         'yahoo.es']), 'P_emaildomain'] = 'Yahoo Mail'\ndf_trans.loc[df_trans['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                         'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                         'outlook.es', 'live.com', 'live.fr',\n                                         'hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\ndf_trans.loc[df_trans.P_emaildomain.isin(df_trans.P_emaildomain\\\n                                         .value_counts()[df_trans.P_emaildomain.value_counts() <= 500 ]\\\n                                         .index), 'P_emaildomain'] = \"Others\"\ndf_trans.P_emaildomain.fillna(\"NoInf\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:52:54.069252Z","iopub.execute_input":"2021-06-27T04:52:54.069600Z","iopub.status.idle":"2021-06-27T04:52:54.833682Z","shell.execute_reply.started":"2021-06-27T04:52:54.069566Z","shell.execute_reply":"2021-06-27T04:52:54.832795Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Plotting P-Email Domain","metadata":{}},{"cell_type":"code","source":"ploting_cnt_amt(df_trans, 'P_emaildomain')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### R-Email Domain plot distribution\n* I will group all e-mail domains by the respective enterprises.\n* I will set as \"Others\" all values with less than 300 entries.","metadata":{}},{"cell_type":"code","source":"df_trans.loc[df_trans['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n\ndf_trans.loc[df_trans['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                             'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                             'yahoo.es']), 'R_emaildomain'] = 'Yahoo Mail'\ndf_trans.loc[df_trans['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                             'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                             'outlook.es', 'live.com', 'live.fr',\n                                             'hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\ndf_trans.loc[df_trans.R_emaildomain.isin(df_trans.R_emaildomain\\\n                                         .value_counts()[df_trans.R_emaildomain.value_counts() <= 300 ]\\\n                                         .index), 'R_emaildomain'] = \"Others\"\ndf_trans.R_emaildomain.fillna(\"NoInf\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:53:14.432888Z","iopub.execute_input":"2021-06-27T04:53:14.433209Z","iopub.status.idle":"2021-06-27T04:53:14.884135Z","shell.execute_reply.started":"2021-06-27T04:53:14.433178Z","shell.execute_reply":"2021-06-27T04:53:14.883295Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"ploting_cnt_amt(df_trans, 'R_emaildomain')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see a very similiar distribution in both email and domain features.","metadata":{}},{"cell_type":"markdown","source":"### C1-C14 features","metadata":{}},{"cell_type":"code","source":"resumetable(df_trans[['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8','C9', 'C10', 'C11', 'C12', 'C13', 'C14']])","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:55:02.709069Z","iopub.execute_input":"2021-06-27T04:55:02.709509Z","iopub.status.idle":"2021-06-27T04:55:03.143481Z","shell.execute_reply.started":"2021-06-27T04:55:02.709470Z","shell.execute_reply":"2021-06-27T04:55:03.142654Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Dataset Shape: (590540, 14)\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"   Name   dtypes  Missing  Uniques  First Value  Second Value  Third Value  \\\n0    C1  float16        0     1495          1.0           1.0          1.0   \n1    C2  float16        0     1167          1.0           1.0          1.0   \n2    C3  float16        0       27          0.0           0.0          0.0   \n3    C4  float16        0     1223          0.0           0.0          0.0   \n4    C5  float16        0      319          0.0           0.0          0.0   \n5    C6  float16        0     1291          1.0           1.0          1.0   \n6    C7  float16        0     1069          0.0           0.0          0.0   \n7    C8  float16        0     1130          0.0           0.0          0.0   \n8    C9  float16        0      205          1.0           0.0          1.0   \n9   C10  float16        0     1122          0.0           0.0          0.0   \n10  C11  float16        0     1343          2.0           1.0          1.0   \n11  C12  float16        0     1066          0.0           0.0          0.0   \n12  C13  float16        0     1464          1.0           1.0          1.0   \n13  C14  float16        0     1108          1.0           1.0          1.0   \n\n    Entropy  \n0      2.72  \n1      2.75  \n2      0.04  \n3      1.12  \n4      2.06  \n5      2.52  \n6      0.71  \n7      1.25  \n8      2.62  \n9      1.23  \n10     2.17  \n11     0.95  \n12     4.66  \n13     2.67  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>dtypes</th>\n      <th>Missing</th>\n      <th>Uniques</th>\n      <th>First Value</th>\n      <th>Second Value</th>\n      <th>Third Value</th>\n      <th>Entropy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>C1</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>1495</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>C2</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>1167</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C3</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>27</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>C4</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>1223</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>C5</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>319</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.06</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>C6</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>1291</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.52</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>C7</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>1069</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.71</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>C8</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>1130</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.25</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>C9</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>205</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.62</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>C10</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>1122</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.23</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>C11</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>1343</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.17</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>C12</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>1066</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.95</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>C13</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>1464</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.66</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>C14</td>\n      <td>float16</td>\n      <td>0</td>\n      <td>1108</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.67</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_trans[['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8','C9', 'C10', 'C11', 'C12', 'C13', 'C14']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:54:52.681905Z","iopub.execute_input":"2021-06-27T04:54:52.682238Z","iopub.status.idle":"2021-06-27T04:54:53.674395Z","shell.execute_reply.started":"2021-06-27T04:54:52.682207Z","shell.execute_reply":"2021-06-27T04:54:53.673469Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"             C1        C2        C3        C4        C5        C6        C7  \\\ncount  590540.0  590540.0  590540.0  590540.0  590540.0  590540.0  590540.0   \nmean        NaN       NaN       0.0       NaN       NaN       NaN       NaN   \nstd         NaN       NaN       0.0       NaN       NaN       NaN       NaN   \nmin         0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n25%         1.0       1.0       0.0       0.0       0.0       1.0       0.0   \n50%         1.0       1.0       0.0       0.0       0.0       1.0       0.0   \n75%         3.0       3.0       0.0       0.0       1.0       2.0       0.0   \nmax      4684.0    5692.0      26.0    2252.0     349.0    2252.0    2256.0   \n\n             C8        C9       C10       C11       C12       C13       C14  \ncount  590540.0  590540.0  590540.0  590540.0  590540.0  590540.0  590540.0  \nmean        NaN       NaN       NaN       NaN       NaN       NaN       NaN  \nstd         NaN       0.0       NaN       NaN       NaN       NaN       NaN  \nmin         0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n25%         0.0       0.0       0.0       1.0       0.0       1.0       1.0  \n50%         0.0       1.0       0.0       1.0       0.0       3.0       1.0  \n75%         0.0       2.0       0.0       2.0       0.0      12.0       2.0  \nmax      3332.0     210.0    3256.0    3188.0    3188.0    2918.0    1429.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>C5</th>\n      <th>C6</th>\n      <th>C7</th>\n      <th>C8</th>\n      <th>C9</th>\n      <th>C10</th>\n      <th>C11</th>\n      <th>C12</th>\n      <th>C13</th>\n      <th>C14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>590540.0</td>\n      <td>590540.0</td>\n      <td>590540.0</td>\n      <td>590540.0</td>\n      <td>590540.0</td>\n      <td>590540.0</td>\n      <td>590540.0</td>\n      <td>590540.0</td>\n      <td>590540.0</td>\n      <td>590540.0</td>\n      <td>590540.0</td>\n      <td>590540.0</td>\n      <td>590540.0</td>\n      <td>590540.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4684.0</td>\n      <td>5692.0</td>\n      <td>26.0</td>\n      <td>2252.0</td>\n      <td>349.0</td>\n      <td>2252.0</td>\n      <td>2256.0</td>\n      <td>3332.0</td>\n      <td>210.0</td>\n      <td>3256.0</td>\n      <td>3188.0</td>\n      <td>3188.0</td>\n      <td>2918.0</td>\n      <td>1429.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_trans.loc[df_trans.C1.isin(df_trans.C1.value_counts()[df_trans.C1.value_counts() <= 400 ].index), 'C1'] = \"Others\"","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:55:34.784979Z","iopub.execute_input":"2021-06-27T04:55:34.785296Z","iopub.status.idle":"2021-06-27T04:55:35.205398Z","shell.execute_reply.started":"2021-06-27T04:55:34.785264Z","shell.execute_reply":"2021-06-27T04:55:35.204539Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### C1 Distribution Plot","metadata":{}},{"cell_type":"code","source":"ploting_cnt_amt(df_trans, 'C1')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_trans.loc[df_trans.C2.isin(df_trans.C2.value_counts()[df_trans.C2.value_counts() <= 350 ].index), 'C2'] = \"Others\"","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:56:21.574240Z","iopub.execute_input":"2021-06-27T04:56:21.574598Z","iopub.status.idle":"2021-06-27T04:56:22.035259Z","shell.execute_reply.started":"2021-06-27T04:56:21.574561Z","shell.execute_reply":"2021-06-27T04:56:22.034370Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"ploting_cnt_amt(df_trans, 'C2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TimeDelta Feature\n* Let's see if the frauds have some specific hour that has highest % of frauds.","metadata":{}},{"cell_type":"code","source":"import datetime\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ndf_trans[\"Date\"] = df_trans['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\ndf_trans['_Weekdays'] = df_trans['Date'].dt.dayofweek\ndf_trans['_Hours'] = df_trans['Date'].dt.hour\ndf_trans['_Days'] = df_trans['Date'].dt.day","metadata":{"execution":{"iopub.status.busy":"2021-06-27T04:57:46.818846Z","iopub.execute_input":"2021-06-27T04:57:46.819200Z","iopub.status.idle":"2021-06-27T04:57:47.953153Z","shell.execute_reply.started":"2021-06-27T04:57:46.819169Z","shell.execute_reply":"2021-06-27T04:57:47.952069Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"ploting_cnt_amt(df_trans, '_Days')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting Weekdays Distributions","metadata":{}},{"cell_type":"code","source":"ploting_cnt_amt(df_trans, '_Weekdays')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting Hours Distributions","metadata":{}},{"cell_type":"code","source":"ploting_cnt_amt(df_trans, '_Hours')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modelling","metadata":{}},{"cell_type":"code","source":"df_trans = pd.read_csv('../input/train_transaction.csv')\ndf_test_trans = pd.read_csv('../input/test_transaction.csv')\n\ndf_id = pd.read_csv('../input/train_identity.csv')\ndf_test_id = pd.read_csv('../input/test_identity.csv')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ndf_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\ndf_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n\nprint(df_train.shape)\nprint(df_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    df_train[c + '_bin'] = df_train[c].map(emails)\n    df_test[c + '_bin'] = df_test[c].map(emails)\n    \n    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n    \n    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for f in df_train.drop('isFraud', axis=1).columns:\n    if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n        df_train[f] = lbl.transform(list(df_train[f].values))\n        df_test[f] = lbl.transform(list(df_test[f].values))   ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Trans_min_mean'] = df_train['TransactionAmt'] - df_train['TransactionAmt'].mean()\ndf_train['Trans_min_std'] = df_train['Trans_min_mean'] / df_train['TransactionAmt'].std()\ndf_test['Trans_min_mean'] = df_test['TransactionAmt'] - df_test['TransactionAmt'].mean()\ndf_test['Trans_min_std'] = df_test['Trans_min_mean'] / df_test['TransactionAmt'].std()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['TransactionAmt_to_mean_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('mean')\ndf_train['TransactionAmt_to_mean_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('mean')\ndf_train['TransactionAmt_to_std_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('std')\ndf_train['TransactionAmt_to_std_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ndf_test['TransactionAmt_to_mean_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('mean')\ndf_test['TransactionAmt_to_mean_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('mean')\ndf_test['TransactionAmt_to_std_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('std')\ndf_test['TransactionAmt_to_std_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('std')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['TransactionAmt'] = np.log(df_train['TransactionAmt'])\ndf_test['TransactionAmt'] = np.log(df_test['TransactionAmt'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['isFraud'] = 'test'\ndf = pd.concat([df_train, df_test], axis=0, sort=False )\ndf = df.reset_index()\ndf = df.drop('index', axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def PCA_change(df, cols, n_components, prefix='PCA_', rand_seed=4):\n    pca = PCA(n_components=n_components, random_state=rand_seed)\n\n    principalComponents = pca.fit_transform(df[cols])\n\n    principalDf = pd.DataFrame(principalComponents)\n\n    df.drop(cols, axis=1, inplace=True)\n\n    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n\n    df = pd.concat([df, principalDf], axis=1)\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mas_v = df_train.columns[55:394]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import minmax_scale\nfrom sklearn.decomposition import PCA\n# from sklearn.cluster import KMeans\n\nfor col in mas_v:\n    df[col] = df[col].fillna((df[col].min() - 2))\n    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n\n    \ndf = PCA_change(df, mas_v, prefix='PCA_V_', n_components=30)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = reduce_mem_usage(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = df_train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT',#'Card_ID'\n                                                     ],axis=1)\ny_train = df_train.sort_values('TransactionDT')['isFraud'].astype(bool)\n\nX_test = df_test.sort_values('TransactionDT').drop(['TransactionDT',\n                                                    #'Card_ID'\n                                                   ],axis=1)\ndel df_train\ndf_test = df_test[[\"TransactionDT\"]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold,TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import plot_importance\nfrom sklearn.metrics import make_scorer\n\nimport time\ndef objective(params):\n    time1 = time.time()\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'subsample': \"{:.2f}\".format(params['subsample']),\n        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 7\n    count=1\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n    tss = TimeSeriesSplit(n_splits=FOLDS)\n    y_preds = np.zeros(sample_submission.shape[0])\n    y_oof = np.zeros(X_train.shape[0])\n    score_mean = 0\n    for tr_idx, val_idx in tss.split(X_train, y_train):\n        clf = xgb.XGBClassifier(\n            n_estimators=600, random_state=4, verbose=True, \n            tree_method='gpu_hist', \n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr)\n        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n        #print(y_pred_train)\n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n        # plt.show()\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean / FOLDS)\n\n\nspace = {\n    # The maximum depth of a tree, same as GBM.\n    # Used to control over-fitting as higher depth will allow model \n    # to learn relations very specific to a particular sample.\n    # Should be tuned using CV.\n    # Typical values: 3-10\n    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n    \n    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n    # (meaning pulling weights to 0). It can be more useful when the objective\n    # is logistic regression since you might need help with feature selection.\n    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n    \n    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n    # approach can be more useful in tree-models where zeroing \n    # features might not make much sense.\n    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n    \n    # eta: Analogous to learning rate in GBM\n    # Makes the model more robust by shrinking the weights on each step\n    # Typical final values to be used: 0.01-0.2\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    \n    # colsample_bytree: Similar to max_features in GBM. Denotes the \n    # fraction of columns to be randomly samples for each tree.\n    # Typical values: 0.5-1\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n    \n    # A node is split only when the resulting split gives a positive\n    # reduction in the loss function. Gamma specifies the \n    # minimum loss reduction required to make a split.\n    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n    'gamma': hp.uniform('gamma', 0.01, .7),\n    \n    # more increases accuracy, but may lead to overfitting.\n    # num_leaves: the number of leaf nodes to use. Having a large number \n    # of leaves will improve accuracy, but will also lead to overfitting.\n    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n    \n    # specifies the minimum samples per leaf node.\n    # the minimum number of samples (data) to group into a leaf. \n    # The parameter can greatly assist with overfitting: larger sample\n    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n    \n    # subsample: represents a fraction of the rows (observations) to be \n    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n    # in their paper A Scalable Tree Boosting System recommend \n    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n    \n    # randomly select a fraction of the features.\n    # feature_fraction: controls the subsampling of features used\n    # for training (as opposed to subsampling the actual training data in \n    # the case of bagging). Smaller fractions reduce overfitting.\n    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n    \n    # randomly bag or subsample training data.\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n    \n    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n    # of the training data. Both values need to be set for bagging to be used.\n    # The frequency controls how often (iteration) bagging is used. Smaller\n    # fractions and frequencies reduce overfitting.\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=27)\n\n# Print best parameters\nbest_params = space_eval(space, best)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"BEST PARAMS: \", best_params)\n\nbest_params['max_depth'] = int(best_params['max_depth'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = xgb.XGBClassifier(\n    n_estimators=300,\n    **best_params,\n    tree_method='gpu_hist'\n)\n\nclf.fit(X_train, y_train)\n\ny_preds = clf.predict_proba(X_test)[:,1] ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_important = clf.get_booster().get_score(importance_type=\"weight\")\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n\n# Top 10 features\ndata.head(20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['isFraud'] = y_preds\nsample_submission.to_csv('XGB_hypopt_model.csv')","metadata":{},"execution_count":null,"outputs":[]}]}
